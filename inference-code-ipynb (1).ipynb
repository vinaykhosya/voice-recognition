{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":11328246,"datasetId":7086200,"databundleVersionId":11753324},{"sourceType":"datasetVersion","sourceId":11456691,"datasetId":7178471,"databundleVersionId":11898313},{"sourceType":"datasetVersion","sourceId":11456959,"datasetId":7178644,"databundleVersionId":11898610},{"sourceType":"datasetVersion","sourceId":11458569,"datasetId":7173894,"databundleVersionId":11900353}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch numpy librosa soundfile transformers gradio","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport librosa\nimport json\nimport soundfile as sf\nimport torch.nn as nn\nfrom transformers import HubertModel\nfrom torch.nn import functional as F\nimport os\nimport gradio as gr\n\n# Constants (matching those used during training)\nSAMPLE_RATE = 16000\nMAX_AUDIO_LENGTH = 6 * SAMPLE_RATE  # 6 seconds at 16kHz\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define the model class (same as in your training code)\nclass HubertForAudioClassification(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.hubert = HubertModel.from_pretrained(model_name)\n        self.classifier = nn.Linear(self.hubert.config.hidden_size, num_labels)\n        self.num_labels = num_labels\n        \n    def forward(self, input_values, attention_mask=None, labels=None):\n        outputs = self.hubert(input_values=input_values, attention_mask=attention_mask)\n        hidden_states = outputs.last_hidden_state\n        pooled_output = torch.mean(hidden_states, dim=1)\n        logits = self.classifier(pooled_output)\n        \n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            \n        return {\"loss\": loss, \"logits\": logits} if loss is not None else logits\n\ndef process_audio_for_inference(audio_data, sr, max_length=MAX_AUDIO_LENGTH):\n    \"\"\"\n    Process audio data for inference\n    \"\"\"\n    try:\n        # Ensure audio is mono\n        if len(audio_data.shape) > 1:\n            audio_data = audio_data.mean(axis=1)\n        \n        # Resample if needed\n        if sr != SAMPLE_RATE:\n            audio_data = librosa.resample(y=audio_data, orig_sr=sr, target_sr=SAMPLE_RATE)\n        \n        # Handle length (trim or pad)\n        if len(audio_data) > max_length:\n            audio_data = audio_data[:max_length]\n        else:\n            padding = max_length - len(audio_data)\n            audio_data = np.pad(audio_data, (0, padding), 'constant')\n        \n        # Normalize\n        audio_data = audio_data / (np.max(np.abs(audio_data)) + 1e-6)\n        \n        return audio_data.astype(np.float32)\n    \n    except Exception as e:\n        print(f\"Error processing audio data: {e}\")\n        return np.zeros(max_length, dtype=np.float32)\n\ndef predict_audio_from_gradio(audio_input, model, label_mapping):\n    \"\"\"\n    Run inference on audio input from Gradio\n    \n    Args:\n        audio_input: Tuple of (sample_rate, audio_data) from Gradio\n        model: Loaded HuBERT classification model\n        label_mapping: Dictionary mapping class indices to class names\n        \n    Returns:\n        Dictionary with prediction results and chart data\n    \"\"\"\n    sr, audio_data = audio_input\n    \n    # Process the audio\n    processed_audio = process_audio_for_inference(audio_data, sr)\n    \n    # Convert to tensor and add batch dimension\n    input_values = torch.tensor(processed_audio, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n    \n    # Create attention mask (all ones, same shape as input_values)\n    attention_mask = torch.ones_like(input_values).to(DEVICE)\n    \n    # Set model to evaluation mode\n    model.eval()\n    \n    # Run inference\n    with torch.no_grad():\n        outputs = model(input_values=input_values, attention_mask=attention_mask)\n    \n    # Handle different output types\n    if isinstance(outputs, dict):\n        logits = outputs[\"logits\"]\n    else:\n        logits = outputs\n    \n    # Get predictions\n    probabilities = F.softmax(logits, dim=1)[0]\n    predicted_class_idx = torch.argmax(probabilities).item()\n    predicted_class = label_mapping[str(predicted_class_idx)]\n    confidence = probabilities[predicted_class_idx].item()\n    \n    # Get all probabilities for chart\n    all_probs = {label_mapping[str(i)]: float(probabilities[i].item()) for i in range(len(label_mapping))}\n    \n    # Sort probabilities for better visualization\n    sorted_probs = sorted(all_probs.items(), key=lambda x: x[1], reverse=True)\n    labels = [item[0] for item in sorted_probs]\n    values = [item[1] for item in sorted_probs]\n    \n    # Create result message\n    result_message = f\"Predicted: {predicted_class} (Confidence: {confidence:.2f})\"\n    \n    return {\n        \"prediction\": result_message,\n        \"chart\": (labels, values)\n    }\n\ndef create_gradio_interface():\n    # Model and label mapping paths - update these to your actual paths\n    model_path = \"model_state_dict.pt\"  # Update this path\n    label_path = \"label_mapping.json\"   # Update this path\n    \n    # Load the label mapping\n    with open(label_path, 'r') as f:\n        label_mapping = json.load(f)\n    \n    # Initialize the model\n    num_labels = len(label_mapping)\n    model = HubertForAudioClassification(\"facebook/hubert-base-ls960\", num_labels)\n    \n    # Load model weights\n    state_dict = torch.load(model_path, map_location=DEVICE)\n    model.load_state_dict(state_dict)\n    model = model.to(DEVICE)\n    \n    # Define the prediction function\n    def predict(audio):\n        if audio is None:\n            return {\"prediction\": \"No audio provided\", \"chart\": ([], [])}\n        \n        result = predict_audio_from_gradio(audio, model, label_mapping)\n        return result\n    \n    # Create Gradio interface\n    with gr.Blocks() as demo:\n        gr.Markdown(\"# Audio Classification with HuBERT\")\n        \n        with gr.Row():\n            audio_input = gr.Audio(sources=[\"microphone\", \"upload\"], type=\"numpy\", label=\"Audio Input\")\n        \n        with gr.Row():\n            predict_btn = gr.Button(\"Predict\")\n        \n        with gr.Row():\n            prediction_output = gr.Textbox(label=\"Prediction\")\n        \n        with gr.Row():\n            chart_output = gr.BarPlot(\n                x=\"Class\", \n                y=\"Probability\",\n                title=\"Class Probabilities\",\n                x_title=\"Class\",\n                y_title=\"Probability\",\n                height=400,\n                width=600\n            )\n        \n        # Set up the click event\n        predict_btn.click(\n            fn=predict, \n            inputs=[audio_input], \n            outputs={\n                \"prediction\": prediction_output,\n                \"chart\": chart_output\n            }\n        )\n    \n    return demo\n\nif __name__ == \"__main__\":\n    demo = create_gradio_interface()\n    demo.launch(share=True)  # Set share=False if you don't want a public link","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}