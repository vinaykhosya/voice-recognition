{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "059c9f3d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-18T19:49:44.788877Z",
     "iopub.status.busy": "2025-04-18T19:49:44.788619Z",
     "iopub.status.idle": "2025-04-18T19:51:56.534503Z",
     "shell.execute_reply": "2025-04-18T19:51:56.533636Z"
    },
    "papermill": {
     "duration": 131.75047,
     "end_time": "2025-04-18T19:51:56.535759",
     "exception": false,
     "start_time": "2025-04-18T19:49:44.785289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 19:49:56.345834: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745005796.590872      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745005796.672349      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YAMNet model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745005812.838787      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1745005812.839524      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "category\n",
      "human             100\n",
      "crow               24\n",
      "insects            24\n",
      "sheep              20\n",
      "crickets           20\n",
      "cow                20\n",
      "cat                19\n",
      "dog                18\n",
      "frog               17\n",
      "chirping_birds     16\n",
      "hen                16\n",
      "pig                16\n",
      "Name: count, dtype: int64\n",
      "Balanced class distribution:\n",
      "category\n",
      "dog               16\n",
      "chirping_birds    16\n",
      "crow              16\n",
      "sheep             16\n",
      "frog              16\n",
      "cow               16\n",
      "insects           16\n",
      "hen               16\n",
      "pig               16\n",
      "cat               16\n",
      "crickets          16\n",
      "human             16\n",
      "Name: count, dtype: int64\n",
      "Label mapping:\n",
      "0: cat\n",
      "1: chirping_birds\n",
      "2: cow\n",
      "3: crickets\n",
      "4: crow\n",
      "5: dog\n",
      "6: frog\n",
      "7: hen\n",
      "8: human\n",
      "9: insects\n",
      "10: pig\n",
      "11: sheep\n",
      "Number of training samples: 153\n",
      "Number of validation samples: 39\n",
      "Applying 10x augmentation to training data...\n",
      "Extracting YAMNet embeddings for training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745005846.797703      63 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting YAMNet embeddings for validation data...\n",
      "Number of augmented training samples: 1530\n",
      "Number of validation samples: 39\n",
      "Starting training...\n",
      "Epoch 1/5 | Train Loss: 2.4371 | Train Acc: 0.1765 | Val Loss: 2.4960 | Val Acc: 0.3846\n",
      "Epoch 2/5 | Train Loss: 2.2649 | Train Acc: 0.5516 | Val Loss: 2.4070 | Val Acc: 0.5897\n",
      "Epoch 3/5 | Train Loss: 2.1383 | Train Acc: 0.6595 | Val Loss: 2.2562 | Val Acc: 0.6923\n",
      "Epoch 4/5 | Train Loss: 2.0687 | Train Acc: 0.7124 | Val Loss: 2.2146 | Val Acc: 0.7179\n",
      "Epoch 5/5 | Train Loss: 2.0344 | Train Acc: 0.7163 | Val Loss: 2.2077 | Val Acc: 0.7179\n",
      "Training completed! Best validation accuracy: 0.7179\n",
      "Error processing audio file: Error opening '/kaggle/input/sjc-js/mixkit-dog-barking-twice-1.wav': System error.\n",
      "Predicted class: insects\n",
      "Confidence: 0.0855\n",
      "All class probabilities:\n",
      "  insects: 0.0855\n",
      "  sheep: 0.0851\n",
      "  chirping_birds: 0.0847\n",
      "  human: 0.0845\n",
      "  cow: 0.0841\n",
      "  cat: 0.0839\n",
      "  hen: 0.0832\n",
      "  pig: 0.0830\n",
      "  crow: 0.0826\n",
      "  dog: 0.0815\n",
      "  crickets: 0.0813\n",
      "  frog: 0.0807\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import soundfile as sf\n",
    "import io\n",
    "import ast\n",
    "import os\n",
    "import librosa\n",
    "import json\n",
    "from collections import Counter\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "MAX_AUDIO_LENGTH = 6 * SAMPLE_RATE\n",
    "\n",
    "class YAMNetForAudioClassification(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        # We'll load YAMNet embedding model separately\n",
    "        # Only need to create the classifier layers here\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 512),  # YAMNet outputs 1024-dim embeddings\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_labels)\n",
    "        )\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "    def forward(self, embeddings, labels=None):\n",
    "        logits = self.classifier(embeddings)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else logits\n",
    "\n",
    "\n",
    "# Load YAMNet model\n",
    "def load_yamnet():\n",
    "    print(\"Loading YAMNet model...\")\n",
    "    yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')\n",
    "    return yamnet_model\n",
    "\n",
    "\n",
    "def get_yamnet_embeddings(audio_data):\n",
    "    \"\"\"Extract embeddings from YAMNet model\"\"\"\n",
    "    scores, embeddings, _ = yamnet_model(audio_data)\n",
    "    # Average the embeddings across time\n",
    "    embedding = tf.reduce_mean(embeddings, axis=0).numpy()\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def augment_audio(audio, augmentation_type=\"all\"):\n",
    "    \"\"\"Apply audio augmentation techniques\"\"\"\n",
    "    if augmentation_type == \"none\" or np.random.random() < 0.1:\n",
    "        return audio\n",
    "    \n",
    "    # Make a copy to avoid modifying original\n",
    "    augmented = np.copy(audio)\n",
    "    \n",
    "    if augmentation_type == \"all\" or augmentation_type == \"noise\":\n",
    "        # Add random noise\n",
    "        if np.random.random() < 0.5:\n",
    "            noise_level = np.random.uniform(0.001, 0.01)\n",
    "            noise = np.random.normal(0, noise_level, len(augmented))\n",
    "            augmented = augmented + noise\n",
    "    \n",
    "    if augmentation_type == \"all\" or augmentation_type == \"pitch\":\n",
    "        # Pitch shift\n",
    "        if np.random.random() < 0.5:\n",
    "            n_steps = np.random.uniform(-3, 3)\n",
    "            augmented = librosa.effects.pitch_shift(augmented, sr=SAMPLE_RATE, n_steps=n_steps)\n",
    "    \n",
    "    if augmentation_type == \"all\" or augmentation_type == \"speed\":\n",
    "        # Time stretching\n",
    "        if np.random.random() < 0.5:\n",
    "            rate = np.random.uniform(0.8, 1.2)\n",
    "            augmented = librosa.effects.time_stretch(augmented, rate=rate)\n",
    "            \n",
    "            # Ensure the length is still correct\n",
    "            if len(augmented) > MAX_AUDIO_LENGTH:\n",
    "                augmented = augmented[:MAX_AUDIO_LENGTH]\n",
    "            elif len(augmented) < MAX_AUDIO_LENGTH:\n",
    "                padding = MAX_AUDIO_LENGTH - len(augmented)\n",
    "                augmented = np.pad(augmented, (0, padding), 'constant')\n",
    "    \n",
    "    if augmentation_type == \"all\" or augmentation_type == \"shift\":\n",
    "        # Time shifting\n",
    "        if np.random.random() < 0.5:\n",
    "            shift_amount = int(np.random.uniform(-SAMPLE_RATE * 0.5, SAMPLE_RATE * 0.5))\n",
    "            augmented = np.roll(augmented, shift_amount)\n",
    "    \n",
    "    # Normalize the audio again\n",
    "    augmented = augmented / (np.max(np.abs(augmented)) + 1e-6)\n",
    "    \n",
    "    return augmented.astype(np.float32)\n",
    "\n",
    "\n",
    "def process_audio(audio_bytes, max_length=MAX_AUDIO_LENGTH, apply_augmentation=False):\n",
    "    try:\n",
    "        audio_data, sr = sf.read(io.BytesIO(audio_bytes))\n",
    "        \n",
    "        if len(audio_data.shape) > 1:\n",
    "            audio_data = audio_data.mean(axis=1)\n",
    "        \n",
    "        if sr != SAMPLE_RATE:\n",
    "            audio_data = librosa.resample(y=audio_data, orig_sr=sr, target_sr=SAMPLE_RATE)\n",
    "        \n",
    "        if len(audio_data) > max_length:\n",
    "            audio_data = audio_data[:max_length]\n",
    "        else:\n",
    "            padding = max_length - len(audio_data)\n",
    "            audio_data = np.pad(audio_data, (0, padding), 'constant')\n",
    "        \n",
    "        audio_data = audio_data / (np.max(np.abs(audio_data)) + 1e-6)\n",
    "        \n",
    "        if apply_augmentation:\n",
    "            audio_data = augment_audio(audio_data)\n",
    "            \n",
    "        # Get YAMNet embeddings\n",
    "        embedding = get_yamnet_embeddings(audio_data)\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio: {e}\")\n",
    "        return np.zeros(1024, dtype=np.float32)  # YAMNet embedding dimension is 1024\n",
    "\n",
    "\n",
    "def process_audio_from_file(file_path, max_length=MAX_AUDIO_LENGTH):\n",
    "    try:\n",
    "        audio_data, sr = sf.read(file_path)\n",
    "        \n",
    "        if len(audio_data.shape) > 1:\n",
    "            audio_data = audio_data.mean(axis=1)\n",
    "        \n",
    "        if sr != SAMPLE_RATE:\n",
    "            audio_data = librosa.resample(y=audio_data, orig_sr=sr, target_sr=SAMPLE_RATE)\n",
    "        \n",
    "        if len(audio_data) > max_length:\n",
    "            audio_data = audio_data[:max_length]\n",
    "        else:\n",
    "            padding = max_length - len(audio_data)\n",
    "            audio_data = np.pad(audio_data, (0, padding), 'constant')\n",
    "        \n",
    "        audio_data = audio_data / (np.max(np.abs(audio_data)) + 1e-6)\n",
    "        \n",
    "        # Get YAMNet embeddings\n",
    "        embedding = get_yamnet_embeddings(audio_data)\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio file: {e}\")\n",
    "        return np.zeros(1024, dtype=np.float32)\n",
    "\n",
    "\n",
    "def create_augmented_dataset(dataframe, augmentation_multiplier=5):\n",
    "    \"\"\"Create a dataset with augmented audio samples\"\"\"\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, row in dataframe.iterrows():\n",
    "        try:\n",
    "            if isinstance(row[\"audio\"], str):\n",
    "                audio_dict = ast.literal_eval(row[\"audio\"])\n",
    "            else:\n",
    "                audio_dict = row[\"audio\"]\n",
    "                \n",
    "            audio_bytes = audio_dict['bytes']\n",
    "            \n",
    "            # Add the original sample\n",
    "            processed_embedding = process_audio(audio_bytes)\n",
    "            embeddings.append(processed_embedding)\n",
    "            labels.append(row[\"label_encoded\"])\n",
    "            \n",
    "            # Add augmented samples\n",
    "            for _ in range(augmentation_multiplier - 1):\n",
    "                augmented_embedding = process_audio(audio_bytes, apply_augmentation=True)\n",
    "                embeddings.append(augmented_embedding)\n",
    "                labels.append(row[\"label_encoded\"])\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating dataset: {e}\")\n",
    "    \n",
    "    return {\n",
    "        \"embeddings\": np.array(embeddings, dtype=np.float32),\n",
    "        \"label\": np.array(labels, dtype=np.int64)\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits = pred.predictions\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    pred_labels = np.argmax(logits, axis=1)\n",
    "    \n",
    "    accuracy = np.mean(pred_labels == pred.label_ids)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(pred.label_ids, pred_labels)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    class_accuracies = {}\n",
    "    for class_id in range(len(np.unique(pred.label_ids))):\n",
    "        class_mask = pred.label_ids == class_id\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_acc = np.mean(pred_labels[class_mask] == class_id)\n",
    "            class_accuracies[f\"class_{class_id}_acc\"] = class_acc\n",
    "    \n",
    "    return {\"accuracy\": accuracy, **class_accuracies}\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"embeddings\": self.embeddings[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    embeddings = torch.tensor(np.stack([item[\"embeddings\"] for item in batch]), dtype=torch.float32)\n",
    "    labels = torch.tensor([item[\"labels\"] for item in batch], dtype=torch.long)\n",
    "    \n",
    "    return {\n",
    "        \"embeddings\": embeddings,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, num_epochs=5, batch_size=32, learning_rate=1e-4):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=learning_rate,\n",
    "        total_steps=num_epochs * len(train_loader),\n",
    "        pct_start=0.1,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            embeddings = batch[\"embeddings\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(embeddings, labels)\n",
    "            loss = outputs[\"loss\"]\n",
    "            logits = outputs[\"logits\"]\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                embeddings = batch[\"embeddings\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                \n",
    "                outputs = model(embeddings, labels)\n",
    "                loss = outputs[\"loss\"]\n",
    "                logits = outputs[\"logits\"]\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model, best_val_acc\n",
    "\n",
    "\n",
    "def predict_audio(audio_path, model, label_mapping):\n",
    "    processed_embedding = process_audio_from_file(audio_path)\n",
    "    input_values = torch.tensor(processed_embedding, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)\n",
    "    \n",
    "    if isinstance(outputs, dict):\n",
    "        logits = outputs[\"logits\"]\n",
    "    else:\n",
    "        logits = outputs\n",
    "        \n",
    "    predicted_class_idx = torch.argmax(logits, dim=1).item()\n",
    "    predicted_class = label_mapping[predicted_class_idx]\n",
    "    \n",
    "    probs = torch.nn.functional.softmax(logits, dim=1)[0]\n",
    "    probability = probs[predicted_class_idx].item()\n",
    "    \n",
    "    all_probs = {label_mapping[i]: probs[i].item() for i in range(len(label_mapping))}\n",
    "    \n",
    "    return predicted_class, probability, all_probs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Load YAMNet model\n",
    "    yamnet_model = load_yamnet()\n",
    "\n",
    "    # Data loading\n",
    "    esc50_path = 'data/train-00000-of-00001-cd782ca55710a2e6.parquet'\n",
    "    esc50_df = pd.read_parquet(\"hf://datasets/mskov/ESC50/\" + esc50_path)\n",
    "\n",
    "    target_categories = [\n",
    "        'dog', 'chirping_birds', 'crow', 'sheep', 'frog', \n",
    "        'cow', 'insects', 'hen', 'pig', 'cat', 'crickets'\n",
    "    ]\n",
    "\n",
    "    esc50_df = esc50_df[['audio', 'category']]\n",
    "    esc50_df = esc50_df[esc50_df['category'].isin(target_categories)]\n",
    "\n",
    "    human_df = pd.read_parquet(\"hf://datasets/pipecat-ai/human_5_all/data/train-00000-of-00001.parquet\")\n",
    "    human_df = human_df[human_df['endpoint_bool'] == True][['audio']].copy()\n",
    "    human_df['category'] = 'human'\n",
    "\n",
    "    # Downsample human class to prevent imbalance\n",
    "    human_df = human_df.sample(100, random_state=seed)\n",
    "\n",
    "    combined_df = pd.concat([esc50_df, human_df], ignore_index=True)\n",
    "\n",
    "    print(f\"Original class distribution:\")\n",
    "    print(combined_df['category'].value_counts())\n",
    "\n",
    "    # Balance dataset\n",
    "    category_counts = combined_df['category'].value_counts()\n",
    "    min_samples = min(category_counts)\n",
    "\n",
    "    balanced_df = pd.DataFrame()\n",
    "    for category in combined_df['category'].unique():\n",
    "        category_df = combined_df[combined_df['category'] == category]\n",
    "        if len(category_df) > min_samples:\n",
    "            category_df = category_df.sample(min_samples, random_state=seed)\n",
    "        balanced_df = pd.concat([balanced_df, category_df], ignore_index=True)\n",
    "\n",
    "    print(f\"Balanced class distribution:\")\n",
    "    print(balanced_df['category'].value_counts())\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    balanced_df[\"label_encoded\"] = label_encoder.fit_transform(balanced_df[\"category\"])\n",
    "    num_labels = len(label_encoder.classes_)\n",
    "\n",
    "    print(f\"Label mapping:\")\n",
    "    label_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "    for i, label in label_mapping.items():\n",
    "        print(f\"{i}: {label}\")\n",
    "\n",
    "    # Split into train/validation sets\n",
    "    train_df, eval_df = train_test_split(balanced_df, test_size=0.2, stratify=balanced_df[\"category\"], random_state=seed)\n",
    "\n",
    "    print(f\"Number of training samples: {len(train_df)}\")\n",
    "    print(f\"Number of validation samples: {len(eval_df)}\")\n",
    "\n",
    "    # Create datasets with augmentation for training\n",
    "    augmentation_multiplier = 10  # Create 10x more samples\n",
    "    print(f\"Applying {augmentation_multiplier}x augmentation to training data...\")\n",
    "    print(\"Extracting YAMNet embeddings for training data...\")\n",
    "    train_data = create_augmented_dataset(train_df, augmentation_multiplier=augmentation_multiplier)\n",
    "    print(\"Extracting YAMNet embeddings for validation data...\")\n",
    "    eval_data = create_augmented_dataset(eval_df, augmentation_multiplier=1)  # No augmentation for eval\n",
    "\n",
    "    print(f\"Number of augmented training samples: {len(train_data['label'])}\")\n",
    "    print(f\"Number of validation samples: {len(eval_data['label'])}\")\n",
    "\n",
    "    # Create custom datasets\n",
    "    train_dataset = CustomDataset(train_data['embeddings'], train_data['label'])\n",
    "    eval_dataset = CustomDataset(eval_data['embeddings'], eval_data['label'])\n",
    "\n",
    "    # Create model\n",
    "    model = YAMNetForAudioClassification(num_labels)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    model, best_val_acc = train_model(\n",
    "        model,\n",
    "        train_dataset,\n",
    "        eval_dataset,\n",
    "        num_epochs=5,\n",
    "        batch_size=32,\n",
    "        learning_rate=5e-5\n",
    "    )\n",
    "    print(f\"Training completed! Best validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    output_dir = f\"./final-yamnet-audio-classifier-{int(time.time())}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{output_dir}/model_state_dict.pt\")\n",
    "\n",
    "    with open(f'{output_dir}/label_mapping.json', 'w') as f:\n",
    "        json.dump(label_mapping, f)\n",
    "\n",
    "    # Test prediction\n",
    "    test_audio_path = \"/kaggle/input/sjc-js/mixkit-dog-barking-twice-1.wav\"\n",
    "    predicted_class, confidence, all_probs = predict_audio(test_audio_path, model, label_mapping)\n",
    "    print(f\"Predicted class: {predicted_class}\")\n",
    "    print(f\"Confidence: {confidence:.4f}\")\n",
    "    print(\"All class probabilities:\")\n",
    "    for label, prob in sorted(all_probs.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {label}: {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7086200,
     "sourceId": 11328246,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 139.502682,
   "end_time": "2025-04-18T19:51:59.601848",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-18T19:49:40.099166",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
